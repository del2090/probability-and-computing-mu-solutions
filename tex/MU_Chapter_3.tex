\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}

\title {Probability and Computing, 2nd Edition \\[2ex] \large Solutions to Chapter 3: Moments and Deviations}
\author{Hahndeul Kim}
\date{July 2025}

\begin{document}

\maketitle
\newpage
\section*{3.1}
$\textbf{E}[X^2]=\sum\limits_{i=1}^n\frac{1}{n}\times i^2 = \frac{(n+1)(2n+1)}{6}$, and $\textbf{E}[X]=\frac{n+1}{2}$.\\
Thus, $\textbf{Var}[X]=\textbf{E}[X^2]-(\textbf{E}[X])^2=\frac{n^2-1}{12}$.
\section*{3.2}
$\textbf{E}[X]=0$, and $\textbf{E}[X^2]=\sum\limits_{i=1}^k \frac{2}{2k+1} \times i^2 = \frac{k(k+1)}{3}$.\\
Thus, $\textbf{Var}[X]=\textbf{E}[X^2]-(\textbf{E}[X])^2=\frac{k(k+1)}{3}$.
\section*{3.3}
The variance of a single die roll is $\frac{35}{12}$ from problem 3.1.
Since all rolls are independent, $\Pr(|X-350| \geq 50) \leq \frac{1}{50^2}\times \frac{35}{12} \times 100 = \frac{7}{60}$.
\section*{3.4}
$\textbf{Var}[cX]=\textbf{E}[(cX-\textbf{E}[cX])^2]=\textbf{E}[c^2X^2 - 2cX\textbf{E}[cX]+(\textbf{E}[cX])^2]$\\
$=c^2(\textbf{E}[X^2] - (\textbf{E}[X])^2)=c^2\textbf{Var}[X]$. $\blacksquare$
\section*{3.5}
$\textbf{Var}[X-Y]=\textbf{E}[((X-Y)-\textbf{E}[X-Y])^2]=\textbf{E}[((X-\textbf{E}[X])-(Y-\textbf{E}[Y]))^2]$\\
$=\textbf{E}[(X-\textbf{E}[X])^2] - 2\textbf{E}[(X-\textbf{E}[X])(Y-\textbf{E}[Y])]+\textbf{E}[(Y-\textbf{E}[Y])^2]$\\
$=\textbf{Var}[X]-\textbf{Cov}[X,Y]+\textbf{Var}[Y]=\textbf{Var}[X]+\textbf{Var}[Y]$ ($X \perp\!\!\!\perp Y$). $\blacksquare$
\section*{3.6}
Let $X_i$ ($1 \leq i \leq k$) be the number of flips after $(i-1)$th head until $i$th head.
Since all flips are independent, the desired variance could be computed as $\sum\limits_{i=1}^k \textbf{Var}[X_i]$.\\
As $X_i\sim Geom(p)$, $\textbf{Var}[X_i]=(1-p)/p^2$ for all $i$. Thus, the desired variance is $k(1-p)/p^2$.
\section*{3.7}
Let $X$ be the number of increases. Then $\Pr(X=k)=\binom{d}{k}p^k(1-p)^{d-k}$.\\
Let the price of the stock after $d$ days be $V$.\\
Then $\textbf{E}[V]=\sum\limits_{k=0}^d qr^k(\frac{1}{r})^{d-k}\binom{d}{k}p^k(1-p)^{d-k}=\sum\limits_{k=0}^dq\binom{d}{k}(pr)^k(\frac{1-p}{r})^{n-k}$.\\
Let $M=pr + (1-p)/r = (1-p+pr^2)/r$. Then\\
$\textbf{E}[V]=M^d\sum\limits_{k=0}^dq\binom{d}{k}(\frac{pr}{M})^k(\frac{1-p}{rM})^{d-k}=M^d\sum\limits_{k=0}^dq\binom{d}{k}(\frac{pr^2}{rM})^k(\frac{1-p}{rM})^{d-k}=M^dq$.\\
Now we compute $\textbf{E}[V^2]=\sum\limits_{k=0}^dq^2 r^{2k}(\frac{1}{r})^{2d-2k}\binom{d}{k}p^k(1-p)^{d-k}$.\\
$\textbf{E}[V^2]=q^2\sum\limits_{k=0}^d\binom{d}{k}(pr^2)^k(\frac{1-p}{r^2})^{d-k}=q^2\left(pr^2+\frac{1-p}{r^2} \right)^d$ (similar to $\textbf{E}[V]$).\\
Thus, $\textbf{Var}[V]=q^2 \left((pr^2 + \frac{1-p}{r^2})^d - (pr + \frac{1-p}{r})^{2d}\right)$.\\
By plugging $q=1$ in, we get the desired result.
\section*{3.8}
Let $X$ be the running time of the given algorithm on input strings of size $n$.
Now, let $M$ be the longest running time of the algorithm among the input strings of size $n$. Then $\Pr(X\geq M) \geq 1/2^n$ by definition.\\
By Markov's inequality, $1/2^n \leq \Pr(X\geq M) \leq \frac{\textbf{E}[X]}{M}$, which leads to $M \leq 2^n \textbf{E}[X]$.
Since $\textbf{E}[X]=O(n^2)$, we get $M=O(n^2 2^n)$.
\section*{3.9}
(a) By linearity of expectations, $\textbf{E}[X^2]=\textbf{E}[\sum\limits_{i=1}^nX_i X]=\sum\limits_{i=1}^n\textbf{E}[X_iX]$.\\
Since $X_i$ are Bernoulli random variables, $\textbf{E}[X_iX]=\Pr(X_i=0)\times 0+\Pr(X_i=1)\times \textbf{E}[X|X_i=1]$. $\blacksquare$\\
(b) Using the equation proven in (a), $\textbf{E}[X^2]=\sum\limits_{i=1}^np\times(1+(n-1)p)=np+n(n-1)p^2$.
Thus, $\textbf{Var}[X]=\textbf{E}[X^2]-(\textbf{E}[X])^2=np+n(n-1)p^2-n^2p^2=np(1-p)$.
\section*{3.10}
Let $X\sim Geom(p)$, and let $Y=1$ if and only if $X=1$ and $Y=0$ otherwise. Then, by Lemma 2.5,
$\textbf{E}[X^3]=\Pr(Y=1)\textbf{E}[X^3|Y=1]+\Pr(Y=0)\textbf{E}[X^3|Y=0]$\\
$=p\times 1 + (1-p)\times \textbf{E}[X^3|Y=0]=p+(1-p)\times\textbf{E}[X^3|X>1]$.\\
Now, by the memoryless property of geometric distributions, $\textbf{E}[X^3|X>1]=\textbf{E}[(X+1)^3]$.
Thus, $\textbf{E}[X^3]=p+(1-p)(\textbf{E}[X^3]+3\textbf{E}[X^2]+3\textbf{E}[X]+1)$.\\
This leads to $\textbf{E}[X^3]=(p^2-6p+6)/p^3$.\\
Similarly, we can find $\textbf{E}[X^4]=(-p^3+14p^2-36p+24)/p^4$.
\section*{3.11}
Let $X=\sum\limits_{i<j} X_{i,j}$, where $X_{i,j}$ is an indicator variable that is $1$ if $a_i$ and $a_j$ are inverted. Then, we compute $\textbf{E}[X^2]$ as:\\
$\textbf{E}[X^2]=\textbf{E}[\sum\limits_{i<j}X_{i,j}^2 +\sum\limits_{|\{i,j,k,l\}|=4}X_{i,j}X_{k,l} + \sum\limits_{i<j<k}2(X_{i,j}X_{j,k}+X_{i,j}X_{i,k}+X_{i,k}X_{j,k})]$
$=\binom{n}{2}\cdot\frac{1}{2}+\binom{n}{4}\cdot\binom{4}{2}\cdot\frac{1}{4}+2\cdot\binom{n}{3}\cdot\left( \frac{1}{6}+\frac{1}{3}+\frac{1}{3}\right)=\frac{n(n-1)(9n^2-5n+10)}{144}$.\\
Since $\textbf{E}[X]=\frac{n(n-1)}{4}$, $\textbf{Var}[X]=\textbf{E}[X^2]-(\textbf{E}[X])^2=\frac{n(n-1)(2n+5)}{72}$.
\section*{3.12}
Let $X$ be a random variable with probability distribution like $\Pr(X=n)=\frac{1}{\zeta (3)n^3}$.\\
Then $\textbf{E}[X]=\sum\limits_{n=1}^\infty n\times\frac{1}{\zeta (3)n^3}=\frac{1}{\zeta(3)}\times\frac{\pi^2}{6}<\infty$.\\
However, since $\textbf{E}[X^2]=\sum\limits_{n=1}^\infty n^2\times\frac{1}{\zeta (3)n^3}=\sum\limits_{n=1}^\infty \frac{1}{n}\times\frac{1}{\zeta (3)}=\infty$ (harmonic series), the variance of $X$ is unbounded.
\section*{3.13}
Let $X$ be a random variable such that $\Pr(X=n)=\frac{1}{\zeta (k+2)n^{k+2}}$.\\
Then, similar to problem 3.12, $\textbf{E}[X^k]$ converges and $\textbf{E}[X^{k+1}]$ diverges.
\section*{3.14}
$\textbf{Var}[\sum\limits_{i=1}^nX_i]=\textbf{E}\left[\left(\sum\limits_{i=1}^n(X_i-\textbf{E}[X_i])\right)^2\right]$
$=\sum\limits_{i=1}^n\textbf{Var}[X_i]+\sum\limits_{i\neq j}\textbf{Cov}[X_i,X_j]=\sum\limits_{i=1}^n\textbf{Var}[X_i]+2\sum\limits_{i=1}^n\sum\limits_{i<j}\textbf{Cov}[X_i,X_j]$. $\blacksquare$
\section*{3.15}
$\textbf{E}[X_iX_j]=\textbf{E}[X_i]\textbf{E}[X_j]$ indicates that $\textbf{Cov}[X_i,X_j]=\textbf{E}[X_iX_j]-\textbf{E}[X_i]\textbf{E}[X_j]=0$.\\
Thus, $\textbf{Var}[X]=\sum\limits_{i=1}^n\textbf{Var}[X_i]$ holds.
\section*{3.16}
Suppose that we want the expectation to be $\mu$.
Then the desired $X$ should satisfy $\Pr(X=k\mu)=1/k$ and $\Pr(X=0)=1-1/k$.
\section*{3.17}
Suppose that we want the expectation to be $\mu$.
Then in order to satisfy $\Pr(|X-\textbf{E}[X]|\geq a)=\frac{\textbf{Var}[X]}{a^2}$, $X$ should satisfy:\\
$\Pr(X=\mu)=p$, $\Pr(X=\mu+a)=(1-p)/2$ and $\Pr(X=\mu-a)=(1-p)/2$.
\section*{3.18}
(a) $\Pr(X-\textbf{E}[X]\geq t\sigma[X])=\Pr[t(X-\textbf{E}[X])+\sigma[X]\geq(t^2+1)\sigma[X]]$\\
$\leq\Pr[(t(X-\textbf{E}[X])+\sigma[X])^2\geq(t^2+1)^2\textbf{Var}[X]]$\\
$\leq \textbf{E}[(t(X-\textbf{E}[X])+\sigma[X])^2]/(t^2+1)^2\textbf{Var}[X]$ (Markov's inequality)\\
=$(t^2\textbf{Var}[X]+\textbf{Var}[X])/(t^2+1)^2\textbf{Var}[X]=1/(t^2+1)$. $\blacksquare$\\
(b) Since probabilities cannot be greater than $1$, we only consider $t \geq 1$.\\
By Chebyshev's inequality, $\Pr(|X-\textbf{E}[X]| \geq t\sigma[X]) \leq 1/t^2 \leq 2/(1+t^2)$. $\blacksquare$
\section*{3.19}
(i) If $\mu=m$, then the claim is trivially valid.\\
(ii) If $\mu < m$, then let $t=|\mu-m|/\sigma$. Then using the result of exercise 3.18(a), $\Pr(X-\mu \geq |\mu-m|) \leq \frac{\sigma^2}{(\mu-m)^2+\sigma^2}$.\\
Now, from $1/2 \leq \Pr(X\geq m) \leq \Pr(X-\mu\geq|\mu-m|) \leq \frac{\sigma^2}{(\mu-m)^2+\sigma^2}$,
we conclude $(\mu-m)^2 \leq \sigma^2$, thus $|\mu-m| \leq \sigma$.\\
(iii) If $\mu > m$, then let $t=|\mu-m|/\sigma$.
Now, substituting $X$ into $-X$ in the result of exercise 3.18(a), we get $\Pr(-X + \textbf{E}[X] \geq t\sigma[X]) \leq 1/(t^2+1)$.\\
This leads to $\Pr(-X+\mu \geq |\mu-m|) \leq \frac{\sigma^2}{(\mu-m)^2+\sigma^2}$.\\
Similarly, from $1/2 \leq \Pr(X\leq m) \leq \Pr(-X+\mu\geq|\mu-m|) \leq \frac{\sigma^2}{(\mu-m)^2+\sigma^2}$,
we conclude $(\mu-m)^2 \leq \sigma^2$, thus $|\mu-m| \leq \sigma$. $\blacksquare$
\section*{3.20}
We first prove $\Pr(Y\neq 0) \leq \textbf{E}[Y]$. This is trivial since\\
$\textbf{E}[Y]-\Pr(Y\neq 0)=\sum\limits_{i=1}^\infty i\Pr(Y=i)-\sum\limits_{i=1}^\infty \Pr(Y=i)=\sum\limits_{i=1}^\infty(i-1)\Pr(Y=i)\geq 0$.\\
Now we prove $\frac{\textbf{E}[Y]^2}{\textbf{E}[Y^2]} \leq \Pr(Y\neq 0)$.
Let $X=Y|Y\neq 0$ such that $\Pr(X=x)=\Pr(Y=x|Y\neq 0)$. Since $(\textbf{E}[X])^2 \leq \textbf{E}[X^2]$, $\textbf{E}[Y|Y\neq 0]^2 \leq \textbf{E}[Y^2|Y\neq 0]$ holds.\\
Now, consider that $\textbf{E}[X]=\textbf{E}[0]\Pr(X=0)+\textbf{E}[X|X\neq0]\Pr(X\neq 0)=\textbf{E}[X|X\neq0]\Pr(X\neq 0)$ is valid for any random variable $X$.\\
Combined with $\textbf{E}[Y|Y\neq 0]^2 \leq \textbf{E}[Y^2|Y\neq 0]$,
$\left(\frac{\textbf{E}[Y]}{\Pr(Y\neq 0)}\right)^2 \leq \frac{\textbf{E}[Y^2]}{\Pr(Y\neq 0)}$ holds. $\blacksquare$
\section*{3.21}
(a) Let $Y=|X-\textbf{E}[X]|$. Then by Markov's inequality,\\
$\Pr(Y>t\sqrt[\leftroot{-2}\uproot{2}k]{\textbf{E}[Y^k]}) =\Pr(Y^k > t^k\textbf{E}[Y^k]) \leq \Pr(Y^k \geq t^k\textbf{E}[Y^k])\leq1/t^k$.\\
(b) If $k$ is odd, then $Y^k \neq (X-\textbf{E}[X])^k$ and $(X-\textbf{E}[X])^k$ may not always be positive.
Therefore, we cannot apply Markov's inequality in this case.
\section*{3.22}
Let $X_i$ be indicator variables that are $1$ if $\pi(i)=i$. Then $X_i\sim Bernoulli(1/n)$.\\
Then $\textbf{Var}[\sum\limits_{i=1}^nX_i]=\sum\limits_{i=1}^n\textbf{Var}[X_i]+2\sum\limits_{i=1}^n\sum\limits_{i<j}\textbf{Cov}[X_i,X_j]$.\\
Now, since $X_i\sim Bernoulli(1/n)$, $\textbf{Var}[X_i]=\frac{1}{n}\left(1-\frac{1}{n}\right)$.\\
$\textbf{Cov}[X_i,X_j]=\textbf{E}[X_iX_j]-\textbf{E}[X_i]\textbf{E}[X_j]$. Since $\textbf{E}[X_iX_j]=\frac{1}{n(n-1)}$, $\textbf{Cov}[X_i,X_j]=\frac{1}{n(n-1)}-\frac{1}{n^2}$.\\
Thus, $\textbf{Var}[\sum\limits_{i=1}^nX_i]=1-\frac{1}{n}+n(n-1)\left(\frac{1}{n(n-1)}-\frac{1}{n^2}\right)=1$.
\section*{3.23}

\end{document}
